{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# create sparkcontext\n",
    "import pyspark\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catch_22.txt MapPartitionsRDD[1] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'catch_22.txt'\n",
    "lines = sc.textFile(file)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'You're a chaplain,' he exclaimed ecstatically. 'I didn't know you were a chaplain.'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miles', 'south', 'of', 'Elba', 'It']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# break lines into words with tokenizer, store as new pipelinedrdd\n",
    "words = lines.flatMap(lambda x: tokenizer.tokenize(x))\n",
    "words.collect()[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mile', 'south', 'of', 'Elba', 'It']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce words to 'root' word\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmed_words = words.map(lambda x: lemmatizer.lemmatize(x))\n",
    "lemmed_words.collect()[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'small', 'obviously', 'could', 'accommodate']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "# remove stop words from text\n",
    "filtered_words = lemmed_words.filter(lambda x: x not in stop_words)\n",
    "filtered_words.collect()[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'small', 'obviously', 'could', 'accommodate']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change all words to lower case\n",
    "lwords = filtered_words.map(lambda x: x.lower())\n",
    "lwords.collect()[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 1), ('small', 1), ('obviously', 1), ('could', 1), ('accommodate', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert words to tuples (needed for reduce step)\n",
    "tuples = lwords.map(lambda x: (x, 1))\n",
    "tuples.collect()[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joseph', 4), ('22', 43), ('copyright', 1), ('c', 87), ('lie', 34)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# condense and perform word count\n",
    "wordcount = tuples.reduceByKey(lambda x,y: x+y)\n",
    "wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wa', 2607),\n",
       " ('i', 2458),\n",
       " ('yossarian', 1476),\n",
       " ('he', 1057),\n",
       " ('colonel', 851),\n",
       " ('the', 823),\n",
       " ('major', 710),\n",
       " ('one', 558),\n",
       " ('back', 545),\n",
       " ('like', 515)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words! - Note lemmatizer does weird things sometimes\n",
    "# thats why we get things like 'wa', it was probably 'was' before lemmatize step\n",
    "wordcount.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
